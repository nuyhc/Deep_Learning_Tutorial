{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트와 시퀀스를 위한 딥러닝\n",
    "텍스트(단어의 시퀀스 또는 문자의 시퀀스), 시계열 또는 일반적인 시퀀스(sequence) 데이터를 처리할 수 있는 딥러닝 모델  \n",
    "- 순환 신경망(RNN, Recurrent Neural Network)\n",
    "- Conv1D (1D 컨브넷)\n",
    "\n",
    "## 텍스트 데이터 다루기\n",
    "컴퓨터 비전이 픽셀에 적용한 패턴 인식(pattern recognition)인 것처럼, 자연어 처리(NLP)를 위한 딥러닝은 단어, 문장, 문단에 적용한 패턴 인식\n",
    "\n",
    "텍스트 원본을 입력으로 사용하지 못하므로, 수치형 텐서로 변환하는 과정을 **텍스트 벡터화(vectorizing text)** 라고 함  \n",
    "1. 텍스트 -> 단어 -> 벡터화\n",
    "2. 텍스트 -> 문자 -> 벡터화\n",
    "3. 텍스트 -> 단어 or 문자 -> n-gram 추출 -> 벡터화\n",
    "\n",
    "텍스트를 나누는 단위(단어, 문자, n-gram)를 **토큰(token)** 이라 함  \n",
    "-> 텍스트를 토큰으로 나누는 작업을 **토큰화(tokenization)** 라고 함  \n",
    "- 원-핫 인코딩(one-hot encoding)\n",
    "- 토큰 임베딩(token embeding)\n",
    "\n",
    "### n-gram과 BoW\n",
    "단어 n-그램(n-gram)은 문장에서 추출한 N개의 연속된 단어 그룹 (단어 대신 문자에도 사용 가능)  \n",
    "`The cat sat on the mat`이라는 문장을 2-그램 집합으로 나타내면,  \n",
    "```python\n",
    "{\"The\", \"The cate\", \"cat\", \"cat sat\", ...}\n",
    "```\n",
    "\n",
    "3-그램 집합으로 나타내면,  \n",
    "```python\n",
    "{\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\", ...}\n",
    "```\n",
    "\n",
    "이런 집합을 각각 2-그램 가방(bag of 2-gram) 또는 3-그램 가방(bag of 3-gram)이라고 함  \n",
    "가방(bag)이란 용어는 다루고자하는 것이 리스트나 시퀀스가 아니라 토큰의 집합이라는 사실을 의미  \n",
    "-> 이 토큰에는 특정한 순서가 없음 -> BoW(Bag-of-Words) 토큰화 방식\n",
    "\n",
    "BoW가 순서가 없는 토큰화 방법(생성된 토큰이 시퀀스가 아니라 집합으로 간주되기 때문)이기 때문에 딥러닝 모델보다는 얕은 학습 방법의 언어 처리 모델에 사용되는 경향이 있음\n",
    "\n",
    "### 단어와 문자의 원-핫 인코딩\n",
    "토큰을 벡터로 변환하는 가장 일반적이고 기본적인 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 수준의 원-핫 인코딩\n",
    "import numpy as np\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index)+1 # 단어마다 고유 인덱스 할당\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros(shape=(len(samples),\n",
    "                          max_length,\n",
    "                          max(token_index.values())+1\n",
    "                          )\n",
    "                   )\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        idx = token_index.get(word)\n",
    "        results[i, j, idx] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 수준 원-핫 인코딩\n",
    "import string\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "characters = string.printable # 출력 가능한 모든 아스키(ASCII) 문자\n",
    "token_index = dict(zip(characters, range(1, len(characters)+1)))\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values())+1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        idx = token_index.get(character)\n",
    "        results[i, j, idx] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 케라스를 사용한 단어 수준의 원-핫 인코딩\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000) # 가장 빈도가 높은 1,000개의 단어만 선택하도록 Tokenizer 객체를 만듬\n",
    "tokenizer.fit_on_texts(samples) # 단어 인덱스 구축\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(samples) # 문자열을 정수 인덱스의 리스트로 변환\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode=\"binary\")\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원-핫 인코딩의 변종 중 하나는 **원-핫 해싱(one-hot hashing)** 기법  \n",
    "-> 어휘 사전에 있는 고유한 토큰의 수가 너무 커서 모두 다루기 어려울 때 사용  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dgdg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2076adaa342e4728b5f632fbf189c39b5d1811ef76d2fb06a694f637be620194"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
