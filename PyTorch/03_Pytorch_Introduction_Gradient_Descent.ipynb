{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graident Descent\n",
    "본질적으로, PyTorch에는 두가지 주요한 특성이 있음\n",
    "- NumPy와 유사하지만 GPU 상에서 실행 가능한 n-차원 텐서(Tensor)\n",
    "- 신경망을 구성하고 학습하는 과정에서의 자동 미분(Automatoc differentiation)\n",
    "\n",
    "## sin(x) 근사하기\n",
    "3차 다항식을 사용해, $y=sin(x)$에 근사(fit)하는 문제 다루기  \n",
    "신경망은 4개의 매개변수를 가지며, 정답과 신경망이 예측한 결과 사이의 유클리드 거리(Euclidean distance)를 최소화하여 임의의 값을 근사할 수 있도록 **경사 하강법(gradient descent)** 을 사용하여 학습 \n",
    "\n",
    "NumPy는 연산 그래프(computation graph)나 딥러닝, 변화도(gradient)에 대해서는 알지 못합니다. 하지만 NumPy 연산을 사용하여 신경망의 순전파 단계와 역전파 단계를 직접 구현함으로써, 3차 다항식이 사인(sine) 함수에 근사하도록 만들 수 있음\n",
    "\n",
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3005.093739696107\n",
      "199 2053.496881150398\n",
      "299 1405.7224600064478\n",
      "399 964.2667462685014\n",
      "499 663.0733590447679\n",
      "599 457.3416379543572\n",
      "699 316.6539523622675\n",
      "799 220.3353372303261\n",
      "899 154.31726097340132\n",
      "999 109.01581916340615\n",
      "1099 77.89474199556972\n",
      "1199 56.49115946165688\n",
      "1299 41.75438824008516\n",
      "1399 31.596674177440697\n",
      "1499 24.5876074303758\n",
      "1599 19.746029990619565\n",
      "1699 16.39817128300095\n",
      "1799 14.080823421353184\n",
      "1899 12.475180918891025\n",
      "1999 11.361581406294208\n",
      "Result: y = 0.04631577819034255 + 0.8323458176324102 x + -0.00799024243570984 x^2 + -0.08986040910531393 x^3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# 무작위 입/출력 데이터 생성\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 무작위로 가중치 초기화\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    # 손실(loss)을 계산하고 출력\n",
    "    loss = np.square(y_pred-y).sum()\n",
    "    if t%100==99: print(t, loss)\n",
    "    # 손실에 따른 a, b, c, d의 변화도(grdient)를 계산하고 역전파\n",
    "    grad_y_pred = 2.0*(y_pred-y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred*x).sum()\n",
    "    grad_c = (grad_y_pred*x**2).sum()\n",
    "    grad_d = (grad_y_pred*x**3).sum()\n",
    "    # 가중치 갱신\n",
    "    a -= learning_rate*grad_a\n",
    "    b -= learning_rate*grad_b\n",
    "    c -= learning_rate*grad_c\n",
    "    d -= learning_rate*grad_d\n",
    "    \n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 tensor(1712.0419)\n",
      "199 tensor(1177.8615)\n",
      "299 tensor(812.1266)\n",
      "399 tensor(561.4324)\n",
      "499 tensor(389.3970)\n",
      "599 tensor(271.2057)\n",
      "699 tensor(189.9148)\n",
      "799 tensor(133.9413)\n",
      "899 tensor(95.3582)\n",
      "999 tensor(68.7333)\n",
      "1099 tensor(50.3409)\n",
      "1199 tensor(37.6223)\n",
      "1299 tensor(28.8181)\n",
      "1399 tensor(22.7176)\n",
      "1499 tensor(18.4864)\n",
      "1599 tensor(15.5489)\n",
      "1699 tensor(13.5076)\n",
      "1799 tensor(12.0879)\n",
      "1899 tensor(11.0997)\n",
      "1999 tensor(10.4112)\n",
      "Result: y = 0.03805118799209595 + 0.8736276626586914 x + -0.006564462557435036 x^2 + -0.09573239833116531 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 무작위로 입/출력 데이터를 생성\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "# 무작위로 가중치를 초기화\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    # 손실(loss)을 계산하고 출력\n",
    "    loss = np.square(y_pred-y).sum()\n",
    "    if t%100==99: print(t, loss)\n",
    "    # 손실에 따른 a, b, c, d의 변화도(grdient)를 계산하고 역전파\n",
    "    grad_y_pred = 2.0*(y_pred-y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred*x).sum()\n",
    "    grad_c = (grad_y_pred*x**2).sum()\n",
    "    grad_d = (grad_y_pred*x**3).sum()\n",
    "    # 가중치 갱신\n",
    "    a -= learning_rate*grad_a\n",
    "    b -= learning_rate*grad_b\n",
    "    c -= learning_rate*grad_c\n",
    "    d -= learning_rate*grad_d\n",
    "    \n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새 autograd Function 정의\n",
    "autograd의 기본 연산자는, 텐서를 조작하는 2개의 함수  \n",
    "1. `forward` 함수는 입력 텐서로부터 출력 텐서를 계산\n",
    "2. `backward` 함수는 어떤 스칼라 값에 대한 출력 텐서의 변화도(gradient)를 전달받고, 동일한 스칼라 값에 대한 입력 텐서의 변화도를 계산\n",
    "\n",
    "`torch.autograd.Function`의 하위클래스(subclass)를 정의하고 `forward`와 `backward` 함수를 구현함으로써 사용자 정의 autograd 연산자를 손쉽게 정의할 수 있음  \n",
    "\n",
    "$P_3(x) = {1 \\over 2}(5x^3 - 3x)$ 3차 르장드르 다항식(Legendre Polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.95834350585938\n",
      "199 144.66018676757812\n",
      "299 100.70249938964844\n",
      "399 71.03519439697266\n",
      "499 50.97850799560547\n",
      "599 37.403133392333984\n",
      "699 28.206867218017578\n",
      "799 21.973188400268555\n",
      "899 17.7457275390625\n",
      "999 14.877889633178711\n",
      "1099 12.931766510009766\n",
      "1199 11.610918045043945\n",
      "1299 10.714258193969727\n",
      "1399 10.10548210144043\n",
      "1499 9.692106246948242\n",
      "1599 9.411375045776367\n",
      "1699 9.220745086669922\n",
      "1799 9.091285705566406\n",
      "1899 9.003361701965332\n",
      "1999 8.943639755249023\n",
      "Result: y = -5.423830273798558e-09 + -2.208526849746704 * P3(1.3320399228078372e-09 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "from turtle import forward\n",
    "\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5*(5*input**3-3*input)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output*1.5*(5*input**2-1)\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# 입/출력 텐서 생성 - 추척은 하지 않음\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "# 임의의 가중치\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # 사용자 정의 Function을 적용하기 위해 Function.apply 메소드를 사용합니다.\n",
    "    # 여기에 'P3'라고 이름을 붙였습니다.\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # 순전파 단계: 연산을 하여 예측값 y를 계산합니다;\n",
    "    # 사용자 정의 autograd 연산을 사용하여 P3를 계산합니다.\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograd를 사용하여 역전파 단계를 계산합니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 갱신합니다.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn모듈\n",
    "연산 그래프와 autograd는 복잡한 연산자를 정의하고 도함수(derivative)를 자동으로 계산하는 매우 강력한 패러다임(paradigm)임  \n",
    "텐서플로우(Tensorflow)에서는, Keras 와 TensorFlow-Slim, TFLearn 같은 패키지들이 연산 그래프를 고수준(high-level)으로 추상화(abstraction)하여  \n",
    "제공하므로 신경망을 구축하는데 유용  \n",
    "-> 파이토치(PyTorch)에서는 nn 패키지가 동일한 목적으로 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 982.2666015625\n",
      "199 653.8624267578125\n",
      "299 436.30999755859375\n",
      "399 292.17388916015625\n",
      "499 196.6658935546875\n",
      "599 133.37106323242188\n",
      "699 91.41831970214844\n",
      "799 63.606956481933594\n",
      "899 45.16706085205078\n",
      "999 32.938541412353516\n",
      "1099 24.827646255493164\n",
      "1199 19.44669532775879\n",
      "1299 15.876077651977539\n",
      "1399 13.506179809570312\n",
      "1499 11.932876586914062\n",
      "1599 10.888101577758789\n",
      "1699 10.194124221801758\n",
      "1799 9.73302173614502\n",
      "1899 9.426568031311035\n",
      "1999 9.222803115844727\n",
      "Result: y = -0.006762427743524313 + 0.8381737470626831 x + 0.0011666310019791126 x^2 + -0.09068938344717026 x^3\n"
     ]
    }
   ],
   "source": [
    "# 입/출력 텐서 생성\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p) # (2000, 1) shape / (3, ) shape\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# MSE\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    # 순전파\n",
    "    y_pred = model(xx)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t%100==99: print(t, loss.item())\n",
    "    # 역전파를 시작하기 전에, 변화도를 0으로 만듬\n",
    "    model.zero_grad()\n",
    "    # 역전파\n",
    "    loss.backward()\n",
    "    # 경사 하강법을 사용해 가중치를 갱신\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate*param.grad\n",
    "\n",
    "linear_layer = model[0]\n",
    "\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1341.424560546875\n",
      "199 1132.794677734375\n",
      "299 941.76953125\n",
      "399 769.7745361328125\n",
      "499 616.8236694335938\n",
      "599 482.7627868652344\n",
      "699 367.20428466796875\n",
      "799 269.63458251953125\n",
      "899 190.11915588378906\n",
      "999 126.16220092773438\n",
      "1099 78.390625\n",
      "1199 45.023807525634766\n",
      "1299 24.294246673583984\n",
      "1399 13.696269989013672\n",
      "1499 9.783488273620605\n",
      "1599 8.96534252166748\n",
      "1699 8.909524917602539\n",
      "1799 8.921245574951172\n",
      "1899 8.922243118286133\n",
      "1999 8.919466018676758\n",
      "Result: y = 0.0004976235213689506 + 0.8562686443328857 x + 0.0004976146738044918 x^2 + -0.09383021295070648 x^3\n"
     ]
    }
   ],
   "source": [
    "# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 입력 텐서 (x, x^2, x^3)를 준비합니다.\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# nn 패키지를 사용하여 모델과 손실 함수를 정의합니다.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산합니다.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계 전에, optimizer 객체를 사용하여 (모델의 학습 가능한 가중치인) 갱신할\n",
    "    # 변수들에 대한 모든 변화도(gradient)를 0으로 만듭니다. 이렇게 하는 이유는 기본적으로 \n",
    "    # .backward()를 호출할 때마다 변화도가 버퍼(buffer)에 (덮어쓰지 않고) 누적되기\n",
    "    # 때문입니다. 더 자세한 내용은 torch.autograd.backward에 대한 문서를 참조하세요.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 매개변수들에 대한 손실의 변화도를 계산합니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # optimizer의 step 함수를 호출하면 매개변수가 갱신됩니다.\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0dda229299321da99d0dbf63d1fbc8b04ca069326b7e356b1394d150d10bd532"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
