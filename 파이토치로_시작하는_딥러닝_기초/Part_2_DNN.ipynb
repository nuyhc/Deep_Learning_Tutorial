{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2 DNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-08-1 Perceptron\n",
    "- 퍼셉트론(Perceptron)\n",
    "- 선형분류기(Linear Classifier)\n",
    "- AND, OR, XOR 게이트"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron\n",
    "- 인공 신경망은, 인간의 뉴런을 본따 만든 신경망  \n",
    "- 입력 신호들의 합이 임계값을 넘으면 신호를 출력하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7055874466896057\n",
      "1000 0.6931471824645996\n",
      "2000 0.6931471824645996\n",
      "3000 0.6931471824645996\n",
      "4000 0.6931471824645996\n",
      "5000 0.6931471824645996\n",
      "6000 0.6931471824645996\n",
      "7000 0.6931471824645996\n",
      "8000 0.6931471824645996\n",
      "9000 0.6931471824645996\n",
      "10000 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "# XOR\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "## nn Layers\n",
    "linear = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "model = torch.nn.Sequential(linear, sigmoid).to(device)\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    \n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step%1000==0:\n",
    "        print(step, cost.item())\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss가 줄어들지 않음 -> 학습이 제대로 진행되지 않음 (XOR은 퍼셉트론으로 해결 불가능)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-08-2 Multi Layer Perceptron\n",
    "- 다중 퍼셉트론(Multi Layer Perceptron)\n",
    "- 오차역전파(Backpropagation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 레이어를 학습 시킬 수 있는 방법 -> 오차역전파 (loss 값의 그라디언트를 최소화하는 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# nn Layers\n",
    "w1 = torch.Tensor(2, 2).to(device)\n",
    "b1 = torch.Tensor(2).to(device)\n",
    "w2 = torch.Tensor(2, 1).to(device)\n",
    "b2 = torch.Tensor(1).to(device)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.34671515226364136\n",
      "1000 0.3467007279396057\n",
      "2000 0.3466891646385193\n",
      "3000 0.34667932987213135\n",
      "4000 0.3466709852218628\n",
      "5000 0.3466639220714569\n",
      "6000 0.346657931804657\n",
      "7000 0.34665271639823914\n",
      "8000 0.34664785861968994\n",
      "9000 0.3466435968875885\n",
      "10000 0.3466399610042572\n"
     ]
    }
   ],
   "source": [
    "lr = 1\n",
    "\n",
    "for step in range(10001):\n",
    "    # forward\n",
    "    l1 = torch.add(torch.matmul(X, w1), b1)\n",
    "    a1 = sigmoid(l1)\n",
    "    l2 = torch.add(torch.matmul(a1, w2), b2)\n",
    "    Y_pred = sigmoid(l2)\n",
    "    # BCE\n",
    "    cost = -torch.mean(Y*torch.log(Y_pred) + (1-Y) * torch.log(1-Y_pred))\n",
    "    \n",
    "    # Back prop (chain rule) (backward)\n",
    "    # loss derivative\n",
    "    d_Y_pred = (Y_pred-Y) / (Y_pred * (1.0-Y_pred) + 1e-7)\n",
    "    \n",
    "    # Layer 2\n",
    "    d_l2 = d_Y_pred * sigmoid_prime(l2)\n",
    "    d_b2 = d_l2\n",
    "    d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n",
    "    \n",
    "    # Layer 1\n",
    "    d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
    "    d_l1 = d_a1 * sigmoid_prime(l1)\n",
    "    d_b1 = d_l1\n",
    "    d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
    "    \n",
    "    # weight update (step)\n",
    "    w1 = w1 - lr*d_w1\n",
    "    b1 = b1 - lr*torch.mean(d_b1, 0)\n",
    "    w2 = w2 - lr*d_w2\n",
    "    b2 = b2 - lr*torch.mean(d_b2, 0)\n",
    "    \n",
    "    if step%1000==0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: xor-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.715934157371521\n",
      "300 0.596375584602356\n",
      "600 0.3768468201160431\n",
      "900 0.35934001207351685\n",
      "1200 0.3544251024723053\n",
      "1500 0.35218489170074463\n",
      "1800 0.3509174585342407\n",
      "2100 0.35010701417922974\n",
      "2400 0.34954583644866943\n",
      "2700 0.34913522005081177\n",
      "3000 0.34882205724716187\n",
      "3300 0.3485758304595947\n",
      "3600 0.3483772873878479\n",
      "3900 0.348213791847229\n",
      "4200 0.3480769991874695\n",
      "4500 0.34796082973480225\n",
      "4800 0.34786105155944824\n",
      "5100 0.3477745056152344\n",
      "5400 0.34769824147224426\n",
      "5700 0.34763121604919434\n",
      "6000 0.34757161140441895\n",
      "6300 0.34751802682876587\n",
      "6600 0.3474700450897217\n",
      "6900 0.34742674231529236\n",
      "7200 0.34738701581954956\n",
      "7500 0.34735098481178284\n",
      "7800 0.34731805324554443\n",
      "8100 0.34728747606277466\n",
      "8400 0.3472594618797302\n",
      "8700 0.347233384847641\n",
      "9000 0.3472091555595398\n",
      "9300 0.3471869230270386\n",
      "9600 0.3471660315990448\n",
      "9900 0.34714627265930176\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "# nn Layers\n",
    "linear1 = torch.nn.Linear(2, 2, bias=True)\n",
    "linear2 = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)\n",
    "# define cost/loss and optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step%300==0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: xor-nn-wide-deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7164230346679688\n",
      "100 0.6931554079055786\n",
      "200 0.6931540966033936\n",
      "300 0.6931527853012085\n",
      "400 0.693151593208313\n",
      "500 0.6931504607200623\n",
      "600 0.6931492686271667\n",
      "700 0.693148136138916\n",
      "800 0.6931470632553101\n",
      "900 0.6931459307670593\n",
      "1000 0.6931447982788086\n",
      "1100 0.6931437253952026\n",
      "1200 0.6931427121162415\n",
      "1300 0.6931416392326355\n",
      "1400 0.6931405067443848\n",
      "1500 0.6931394338607788\n",
      "1600 0.6931382417678833\n",
      "1700 0.6931371688842773\n",
      "1800 0.6931359767913818\n",
      "1900 0.6931347846984863\n",
      "2000 0.6931334733963013\n",
      "2100 0.6931322813034058\n",
      "2200 0.6931309103965759\n",
      "2300 0.6931295394897461\n",
      "2400 0.6931281089782715\n",
      "2500 0.6931265592575073\n",
      "2600 0.6931249499320984\n",
      "2700 0.6931232213973999\n",
      "2800 0.6931214332580566\n",
      "2900 0.6931195855140686\n",
      "3000 0.6931174397468567\n",
      "3100 0.693115234375\n",
      "3200 0.693112850189209\n",
      "3300 0.6931103467941284\n",
      "3400 0.6931076049804688\n",
      "3500 0.6931045055389404\n",
      "3600 0.6931012272834778\n",
      "3700 0.6930975914001465\n",
      "3800 0.6930936574935913\n",
      "3900 0.6930893659591675\n",
      "4000 0.6930843591690063\n",
      "4100 0.6930789947509766\n",
      "4200 0.6930727958679199\n",
      "4300 0.693065881729126\n",
      "4400 0.6930580139160156\n",
      "4500 0.6930490732192993\n",
      "4600 0.6930387020111084\n",
      "4700 0.6930266618728638\n",
      "4800 0.6930125951766968\n",
      "4900 0.6929957866668701\n",
      "5000 0.6929758787155151\n",
      "5100 0.692951500415802\n",
      "5200 0.69292151927948\n",
      "5300 0.6928839087486267\n",
      "5400 0.6928355693817139\n",
      "5500 0.692772388458252\n",
      "5600 0.6926865577697754\n",
      "5700 0.6925660371780396\n",
      "5800 0.6923881769180298\n",
      "5900 0.6921088695526123\n",
      "6000 0.6916301250457764\n",
      "6100 0.690697968006134\n",
      "6200 0.6884748935699463\n",
      "6300 0.6807767748832703\n",
      "6400 0.6191476583480835\n",
      "6500 0.07421649247407913\n",
      "6600 0.010309841483831406\n",
      "6700 0.004796213004738092\n",
      "6800 0.003011580090969801\n",
      "6900 0.0021596732549369335\n",
      "7000 0.001668647164478898\n",
      "7100 0.0013521360233426094\n",
      "7200 0.0011324126971885562\n",
      "7300 0.0009715152555145323\n",
      "7400 0.0008490675827488303\n",
      "7500 0.000752839376218617\n",
      "7600 0.0006754109635949135\n",
      "7700 0.0006118417950347066\n",
      "7800 0.000558803731109947\n",
      "7900 0.0005138349952176213\n",
      "8000 0.00047530914889648557\n",
      "8100 0.000441973126726225\n",
      "8200 0.00041282744496129453\n",
      "8300 0.0003871413064189255\n",
      "8400 0.0003643627860583365\n",
      "8500 0.0003439848660491407\n",
      "8600 0.00032572413329035044\n",
      "8700 0.0003092226979788393\n",
      "8800 0.0002942568971775472\n",
      "8900 0.0002806179691106081\n",
      "9000 0.0002681269543245435\n",
      "9100 0.00025669438764452934\n",
      "9200 0.0002461116237100214\n",
      "9300 0.0002363934472668916\n",
      "9400 0.00022736100072506815\n",
      "9500 0.0002190142695326358\n",
      "9600 0.00021117438154760748\n",
      "9700 0.0002038860257016495\n",
      "9800 0.00019705976592376828\n",
      "9900 0.00019065086962655187\n",
      "10000 0.00018467426707502455\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# nn Layers\n",
    "linear1 = torch.nn.Linear(2, 10, bias=True)\n",
    "linear2 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear3 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear4 = torch.nn.Linear(10, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)\n",
    "# define cost/loss and optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step%100==0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-09-1 ReLU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-09-2 Weight initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-09-3 Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-09-4 Batch Normalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6170c7e636c2dbd0f88cb8f557866518ac6c7d83ee4f2709c7df3161954bfcc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
