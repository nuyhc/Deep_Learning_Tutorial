{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트랜스포머를 활용한 자연어 처리\n",
    "# Chapter 1. 트랜스포머 소개\n",
    "- 시퀀스 모델링(sequence modeling)을 위한 새로운 신경망 아키텍처\n",
    "- 기계 번역 작업의 품질과 훈련 비용 면에서 순환 신경망(RNN)을 능가\n",
    "- 효율적인 전이 학습(transfer learning) 방법인 ULMFiT가 매우 크고 다양한 말뭉치(corpus)에서 LSTM 신경망을 훈련해 매우 적은 양의 레이블링된 데이터로도 최고 수준의 텍스트 분류 모델을 만들어냄을 증명\n",
    "- 트랜스포머 아키텍처와 비지도 학습(unsupervised learning)을 결합\n",
    "  - GPT (Generative Pretrained Transformer)\n",
    "  - BERT (Bidirectional Encoder Representation from Transformers)\n",
    "\n",
    "## 트랜스포머의 새로운 점\n",
    "1. 인코더-디코더 (encoder-decoder) 프레임워크\n",
    "2. 어텐션 매커니즘 (attention mechanism)\n",
    "3. 전이 학습 (transfer learning)\n",
    "\n",
    "## 1.1 인코더-디코더 프레임워크\n",
    "- 트랜스포머 등장 이전, NLP에서는 LSTM 같은 순환 신경망 구조가 최고 수준의 성능을 달성\n",
    "- 피드백 루프가, 텍스트와 같은 순차 데이터를 모델링하는 데 이상적\n",
    "- RNN의 경우, 네트워크를 통과시킨 후 은닉상태(hidden state)라는 벡터를 출력함과 동시에, 출력된 정보를 피드백 루프로 보내 자기 자신에 다시 입력\n",
    "- RNN은 출력한 정보의 일부를 다음 스텝에 사용\n",
    "- 이전 스텝의 정보를 추적하고 이를 사용해 예측을 만듬\n",
    "- 위와 같은 구조는, 인코더-디코더 또는 시퀀스-투-시퀀스(sequence-to-sequence) 구조로 처리하며 입력과 출력이 임의의 길이를 가진 시퀀스일 때 잘 맞음\n",
    "- 인코더는 입력 시퀀스의 정보를 마지막 은닉 상태(last hidden state)라고도 부르는 수치 표현으로 인코딩 -> 디코더로 전달되어 출력 시퀀스가 생성\n",
    "- 인코더의 마지막 은닉 상태가 정보 병목(information bottleneck)이 된다는 약점이 있음\n",
    "- 디코더가 인코더의 모든 은닉 상태에 접근해 병목 현상을 제거 -> 어텐션(attention) 메커니즘\n",
    "\n",
    "## 1.2 어텐션 메커니즘\n",
    "- 입력 시퀀스에서 은닉 상태를 만들지 않고 스텝마다 인코더에서 디코더가 참고할 은닉 상태를 출력한다는 주요 개념에 기초\n",
    "- 어떤 상태를 디코더가 먼저 사용할지 우선순위를 정하는 메커니즘이 필요\n",
    "- 디코더가 모든 디코딩 타임스탭마다 인코더의 각 상태에 다른 가중치 또는 어텐션을 할당\n",
    "- 계산이 순차적으로 수행되며, 입력 시퀀스 전체에 걸쳐 병렬화할 수 없음\n",
    "- 트랜스포머는, 순환을 모두 없애고 **셀프 어텐션(self-attention)**이라는 특별한 형태의 어텐션에 전적으로 의지\n",
    "  - 신경망의 같은 층에 있는 모든 상태에 대해서 어텐션을 작동시키는 방식\n",
    "- 어텐션의 출력은 FF NN에 주입\n",
    "\n",
    "## 1.3 NLP의 전이 학습\n",
    "- 비전 분야의 경우, 전이 학습을 사용해 한 작업에서 훈련한 다음 새로운 작업에 적용하거나 미세 튜닝(fine-tuning)하는 일이 많음\n",
    "- 신경망은, 원래 작업에서 학습한 지식을 사용\n",
    "- 모델은 구조적으로 바디와 헤드로 나눌 수 있음\n",
    "  - 바디: 훈련하는 동안 원래 도메인에서 다양한 특성을 학습하고, 이 가중치를 사용해 새로운 작업을 위한 모델을 초기화\n",
    "- 전통적인 지도 학습(supervised learning)과 비교하면, 전이 학습은 일반적으로 다양한 작업에서 적은 양의 레이블 데이터로 훨씬 효과적으로 훈련하는 높은 품질의 모델을 만듬\n",
    "\n",
    "### ULMFiT\n",
    "- OpenAI 연구원들이, 비지도 사전 훈련에서 추출한 특성을 사용해 높은 성능을 얻음\n",
    "- 다양한 작업에서 사전 훈련된 LSTM 모델을 적용\n",
    "\n",
    "#### 사전 훈련\n",
    "- 이전 단어를 바탕으로 다음 단어를 예측 (언어 모델링, language modeling)\n",
    "\n",
    "#### 도메인 적응\n",
    "- 언어 모델은 대규모 corpus(말뭉치)를 훈련하고, 도메인 내 corpus에 적응 시킴\n",
    "\n",
    "#### 미세튜닝\n",
    "- 언어 모델을 타깃 작업을 위한 분류 층과 함께 미세 튜닝\n",
    "\n",
    "### GPT\n",
    "- 트랜스포머의 아키텍처의 디코더 부분만 사용하고 ULMFiT 같은 언어 모델링 방법을 사용\n",
    "\n",
    "### BERT\n",
    "- 트랜스포머의 아키텍처의 인코더 부분만 사용하고 마스크드 언어 모델링(masked language modeling)이라는 특별한 형태의 언어 모델링을 사용\n",
    "  - 텍스트에서 랜덤하게 마스킹 된 단어를 예측\n",
    "\n",
    "## 1.4 허깅페이스 트랜스포머\n",
    "- 새로운 머신러닝 아키텍처를 새로운 작업에 적용하는 일은, 일반저긍로 다음 단계를 거침\n",
    "  - 1. 모델 아키텍처 구현\n",
    "  - 2. 훈련된 가중치 로드\n",
    "  - 3. 입력을 전처리하고 모델에 전달\n",
    "  - 4. 데이터로더를 구현하고 모델 훈련을 위해 손실 함수와 옵티마이저를 정의\n",
    "\n",
    "## 1.5 트랜스포머 애플리케이션 둘러보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \\\n",
    "\"\"\"\n",
    "Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany.\n",
    "Unfortunately, when I opened the package, I discovered to my horro that I had been sent an action figure of Megatron instead!\n",
    "As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered.\n",
    "Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 텍스트 분류\n",
    "- 원시 텍스트를 미세 튜닝된 모델의 예측으로 변환하기 위해 필요한 모든 단계를 추상화하는 파이프라인을 사용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
